{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5cda41-33b5-4aa7-aaad-c5342ae10bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7671d9b7-664e-4331-b08d-7b78f32f0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "input_csv_path = 'All_Occupations.csv'\n",
    "df_input = pd.read_csv(input_csv_path)\n",
    "urls = df_input['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b65cf9-8b7a-46a4-879f-ef273c41d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle general scraping tasks\n",
    "def scrape_onet_page(url, section_id=None, data_title_main=None, data_title_desc=None, output_columns=None, special_scrape=False):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    data = []\n",
    "    \n",
    "    # General scraping for sections with tables\n",
    "    if section_id and data_title_main and data_title_desc and output_columns:\n",
    "        table_section = soup.select_one(section_id)\n",
    "        if table_section:\n",
    "            for row in table_section.find_all('tr'):\n",
    "                importance_element = row.find('td', {'data-title': data_title_main})\n",
    "                element = row.find('td', {'data-title': data_title_desc})\n",
    "                if importance_element and element:\n",
    "                    importance = importance_element.get_text(strip=True)\n",
    "                    title_element = element.find('b')\n",
    "                    title = title_element.get_text(strip=True) if title_element else ''\n",
    "                    description = element.get_text(strip=True).replace(title, '').strip('— ')\n",
    "                    data.append({\n",
    "                        'URL': url,\n",
    "                        output_columns[0]: importance,\n",
    "                        output_columns[1]: title,\n",
    "                        output_columns[2]: description\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Missing elements in URL: {url}\")\n",
    "    # Special cases like tools, work activities, work context, etc.\n",
    "    elif special_scrape:\n",
    "        if section_id == 'occupation_details':\n",
    "            occupation = soup.select_one('#content > h1 > span:nth-child(1)').get_text(strip=True) if soup.select_one('#content > h1 > span:nth-child(1)') else None\n",
    "            soc_code = soup.select_one('#content > h1 > span:nth-child(2) > div > div:nth-child(1)').get_text(strip=True) if soup.select_one('#content > h1 > span:nth-child(2) > div > div:nth-child(1)') else None\n",
    "            occupation_description = soup.select_one('#content > p:nth-of-type(1)').text.strip() if soup.select_one('#content > p:nth-of-type(1)') else None\n",
    "            sample_of_reported_job_titles = soup.select_one('#content > p:nth-of-type(2)').text.strip() if soup.select_one('#content > p:nth-of-type(2)') else None\n",
    "\n",
    "            data.append({\n",
    "                'url': url,\n",
    "                'occupation': occupation,\n",
    "                'soc_code': soc_code,\n",
    "                'occupation_description': occupation_description,\n",
    "                'sample_of_reported_job_titles': sample_of_reported_job_titles\n",
    "            })\n",
    "        elif section_id == 'technology_skills':\n",
    "            tech_skills_section = soup.select_one('#TechnologySkills > div > ul')\n",
    "            if tech_skills_section:\n",
    "                for item in tech_skills_section.find_all('li'):\n",
    "                    category = item.find('b').get_text(strip=True) if item.find('b') else None\n",
    "                    example_text = item.get_text(strip=True).replace(category, '').strip() if category else item.get_text(strip=True)\n",
    "                    examples = [example.strip() for example in example_text.split(';')]\n",
    "                    for example in examples:\n",
    "                        if example:\n",
    "                            data.append({\n",
    "                                'URL': url,\n",
    "                                'Category': category,\n",
    "                                'Example': example\n",
    "                            })\n",
    "        elif section_id == 'tools':\n",
    "            tools_list = soup.select_one('#ToolsUsed ul')\n",
    "            if tools_list:\n",
    "                for item in tools_list.find_all('li'):\n",
    "                    category_example = item.select_one('div.order-2.flex-grow-1').get_text(strip=True)\n",
    "                    if \"—\" in category_example:\n",
    "                        category, example = category_example.split(\"—\", 1)\n",
    "                        data.append({'Category': category.strip(), 'Example': example.strip(), 'url': url})\n",
    "                    else:\n",
    "                        data.append({'Category': category_example.strip(), 'Example': '', 'url': url})\n",
    "        elif section_id == 'work_activities':\n",
    "            work_activities_section = soup.select_one('#WorkActivities > div > table')\n",
    "            if work_activities_section:\n",
    "                for row in work_activities_section.find('tbody').find_all('tr'):\n",
    "                    importance = row.find('td', {'data-title': 'Importance'}).get_text(strip=True)\n",
    "                    work_activity_element = row.find('td', {'data-title': 'Work Activity'})\n",
    "                    work_activity_title = work_activity_element.find('b').get_text(strip=True)\n",
    "                    description_text = work_activity_element.get_text(strip=True).replace(work_activity_title, '').strip('— ')\n",
    "                    data.append({\n",
    "                        'URL': url,\n",
    "                        'Importance': importance,\n",
    "                        'Work Activity': work_activity_title,\n",
    "                        'Work Activity Description': description_text\n",
    "                    })\n",
    "        elif section_id == 'work_context':\n",
    "            work_context_section = soup.select_one('#WorkContext > div > table')\n",
    "            if work_context_section:\n",
    "                for row in work_context_section.find('tbody').find_all('tr'):\n",
    "                    context_value = row.find('td', {'data-title': 'Context'}).get_text(strip=True)\n",
    "                    work_context_element = row.find('td', {'data-title': 'Work Context'})\n",
    "                    work_context_title = work_context_element.find('b').get_text(strip=True)\n",
    "                    context_description = work_context_element.get_text(strip=True).replace(work_context_title, '').strip('— ')\n",
    "                    data.append({\n",
    "                        'URL': url,\n",
    "                        'Context': context_value,\n",
    "                        'Work Context': work_context_title,\n",
    "                        'Work Context Description': context_description\n",
    "                    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22556d2d-b4ab-4680-9a5e-4ed142cc88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping occupation details\n",
    "occupation_data = []\n",
    "for url in urls:\n",
    "    occupation_data.extend(scrape_onet_page(url, section_id='occupation_details', special_scrape=True))\n",
    "pd.DataFrame(occupation_data).to_csv('occupation_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727da8e-86f8-4c9d-b507-5fc71310e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping technology skills\n",
    "tech_skills_data = []\n",
    "for url in urls:\n",
    "    tech_skills_data.extend(scrape_onet_page(url, section_id='technology_skills', special_scrape=True))\n",
    "pd.DataFrame(tech_skills_data).to_csv('technology_skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bde4b5-db28-40f1-a54a-f61f66ffa82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping tools used\n",
    "tools_data = []\n",
    "for url in urls:\n",
    "    tools_data.extend(scrape_onet_page(url, section_id='tools', special_scrape=True))\n",
    "pd.DataFrame(tools_data).to_csv('tools_used.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751076e6-10ef-4dcd-9406-f8361c5bfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping work activities\n",
    "work_activities_data = []\n",
    "for url in urls:\n",
    "    work_activities_data.extend(scrape_onet_page(url, section_id='work_activities', special_scrape=True))\n",
    "pd.DataFrame(work_activities_data).to_csv('work_activities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c5372-3df8-48c9-be7f-b27795ca6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping work context\n",
    "work_context_data = []\n",
    "for url in urls:\n",
    "    work_context_data.extend(scrape_onet_page(url, section_id='work_context', special_scrape=True))\n",
    "pd.DataFrame(work_context_data).to_csv('work_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9966a37-f9a1-4860-8206-f6e9cc91add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections to scrape with table-based extraction\n",
    "sections = [\n",
    "    {'id': '#Abilities > div > table > tbody', 'main': 'Importance', 'desc': 'Ability', 'output': ['Importance', 'Ability', 'Ability Description'], 'file': 'abilities.csv'},\n",
    "    {'id': '#Interests > table > tbody', 'main': 'Occupational Interest', 'desc': 'Interest', 'output': ['Importance', 'Interest', 'Interest Description'], 'file': 'interests.csv'},\n",
    "    {'id': '#WorkValues > table > tbody', 'main': 'Extent', 'desc': 'Work Value', 'output': ['Extent', 'Work Value', 'Work Value Description'], 'file': 'work_values.csv'},\n",
    "    {'id': '#WorkStyles > div > table > tbody', 'main': 'Importance', 'desc': 'Work Style', 'output': ['Importance', 'Work Style', 'Work Style Description'], 'file': 'work_styles.csv'},\n",
    "    {'id': '#Knowledge > div > table > tbody', 'main': 'Importance', 'desc': 'Knowledge', 'output': ['Importance', 'Knowledge', 'Knowledge Description'], 'file': 'knowledge.csv'},\n",
    "    {'id': '#Skills > div > table > tbody', 'main': 'Importance', 'desc': 'Skill', 'output': ['Importance', 'Skill', 'Skill Description'], 'file': 'skills.csv'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf6a0c-bfcd-45e3-adab-df996ea3eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping tables for each section\n",
    "for section in sections:\n",
    "    section_data = []\n",
    "    for url in urls:\n",
    "        section_data.extend(scrape_onet_page(url, section_id=section['id'], data_title_main=section['main'], data_title_desc=section['desc'], output_columns=section['output']))\n",
    "    pd.DataFrame(section_data).to_csv(section['file'], index=False)\n",
    "    print(f\"Data scraped and saved to '{section['file']}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
